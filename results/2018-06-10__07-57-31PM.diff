diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index b0589f6..e6767ca 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -5,8 +5,10 @@ from gym import spaces
 
 import deepdrive
 import config as c
+from agents.common import get_throttle
 from agents.dagger.agent import Agent
 from agents.dagger.net import MOBILENET_V2_NAME
+from gym_deepdrive.envs.deepdrive_gym_env import Action, DrivingStyle
 from vendor.openai.baselines.ppo2.run_deepdrive import train
 
 
@@ -14,6 +16,7 @@ class BootstrapRLGymEnv(gym.Wrapper):
     def __init__(self, env, dagger_agent):
         super(BootstrapRLGymEnv, self).__init__(env)
         self.dagger_agent = dagger_agent
+        self.previous_obz = None
 
         # One thing we need to do here is to make each action a bi-modal guassian to avoid averaging 50/50 decisions
         # i.e. half the time we veer left, half the time veer right - but on average this is go straight and can run us
@@ -29,7 +32,14 @@ class BootstrapRLGymEnv(gym.Wrapper):
                                             dtype=np.float32)
 
     def step(self, action):
+        if self.env.unwrapped.driving_style == DrivingStyle.STEER_ONLY and self.previous_obz is not None:
+            # Simplifying by only controlling steering. Otherwise, we need to shape rewards so that initial acceleration
+            # is not disincentivized by gforce penalty.
+            action[Action.THROTTLE_INDEX] = get_throttle(actual_speed=self.previous_obz['speed'],
+                                                         target_speed=(8 * 100))
+
         obz, reward, done, info = self.env.step(action)
+        self.previous_obz = obz
         action, net_out = self.dagger_agent.act(obz, reward, done)
         if net_out is None:
             obz = None
@@ -43,7 +53,8 @@ class BootstrapRLGymEnv(gym.Wrapper):
 
 def run(env_id, bootstrap_net_path,
         resume_dir=None, experiment=None, camera_rigs=None, render=False, fps=c.DEFAULT_FPS,
-        should_record=False, is_discrete=False, agent_name=MOBILENET_V2_NAME, is_sync=True):
+        should_record=False, is_discrete=False, agent_name=MOBILENET_V2_NAME, is_sync=True,
+        driving_style=DrivingStyle.NORMAL):
     tf_config = tf.ConfigProto(
         allow_soft_placement=True,
         intra_op_parallelism_threads=1,
@@ -62,7 +73,7 @@ def run(env_id, bootstrap_net_path,
 
         with sess_1.as_default():
             dagger_gym_env = deepdrive.start(experiment, env_id, cameras=camera_rigs, render=render, fps=fps,
-                                             combine_box_action_spaces=True, is_sync=is_sync)
+                                             combine_box_action_spaces=True, is_sync=is_sync, driving_style=driving_style)
 
             dagger_agent = Agent(dagger_gym_env.action_space, sess_1, env=dagger_gym_env.env,
                                  should_record_recovery_from_random_actions=False, should_record=should_record,
@@ -77,7 +88,7 @@ def run(env_id, bootstrap_net_path,
             # Wrap step so we get the pretrained layer activations rather than pixels for our observation
             bootstrap_gym_env = BootstrapRLGymEnv(dagger_gym_env, dagger_agent)
 
-            train(bootstrap_gym_env, num_timesteps=int(18e3), seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete)
+            train(bootstrap_gym_env, seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete)
     #
     # action = deepdrive.action()
     # while not done:
diff --git a/agents/common.py b/agents/common.py
index e69de29..9a578ea 100644
--- a/agents/common.py
+++ b/agents/common.py
@@ -0,0 +1,23 @@
+from __future__ import (absolute_import, division,
+                        print_function, unicode_literals)
+
+import glob
+import os
+
+from future.builtins import (ascii, bytes, chr, dict, filter, hex, input,
+                             int, map, next, oct, open, pow, range, round,
+                             str, super, zip)
+
+import sys
+
+import logs
+import config as c
+
+
+log = logs.get_log(__name__)
+
+
+def get_throttle(actual_speed, target_speed):
+    desired_throttle = abs(target_speed / max(actual_speed, 1e-3))
+    desired_throttle = min(max(desired_throttle, 0.), 1.)
+    return desired_throttle
diff --git a/agents/dagger/agent.py b/agents/dagger/agent.py
index 4731d45..d2c06f9 100644
--- a/agents/dagger/agent.py
+++ b/agents/dagger/agent.py
@@ -10,9 +10,10 @@ import numpy as np
 
 import config as c
 import deepdrive
+from agents.common import get_throttle
 from agents.dagger import net
 from agents.dagger.train.train import resize_images
-from gym_deepdrive.envs.deepdrive_gym_env import Action, Urgency
+from gym_deepdrive.envs.deepdrive_gym_env import Action, DrivingStyle
 from agents.dagger.net import AlexNet, MobileNetV2
 from utils import save_hdf5, download
 import logs
@@ -25,7 +26,7 @@ class Agent(object):
                  should_record=False, net_path=None, use_frozen_net=False, random_action_count=0,
                  non_random_action_count=5, path_follower=False, recording_dir=c.RECORDING_DIR,
                  output_last_hidden=False,
-                 net_name=net.ALEXNET_NAME, urgency=Urgency.NORMAL):
+                 net_name=net.ALEXNET_NAME, driving_style=DrivingStyle.NORMAL):
         np.random.seed(c.RNG_SEED)
         self.action_space = action_space
         self.previous_action = None
@@ -33,7 +34,7 @@ class Agent(object):
         self.step = 0
         self.env = env
         self.net_name = net_name
-        self.urgency = urgency
+        self.driving_style = driving_style
 
         # State for toggling random actions
         self.should_record_recovery_from_random_actions = should_record_recovery_from_random_actions
@@ -146,8 +147,7 @@ class Agent(object):
             # desired_throttle = abs(target_speed / max(actual_speed, 1e-3))
             # desired_throttle = min(max(desired_throttle, 0.), 1.)
             target_speed = 8 * 100
-            desired_throttle = abs(target_speed / max(actual_speed, 1e-3))
-            desired_throttle = min(max(desired_throttle, 0.), 1.)
+            desired_throttle = get_throttle(actual_speed, target_speed)
 
             # if self.previous_net_out:
             #     desired_throttle = 0.2 * self.previous_action.throttle + 0.7 * desired_throttle
@@ -157,19 +157,18 @@ class Agent(object):
         else:
             # AlexNet
 
-            if self.urgency == Urgency.CRUISING:
+            if self.driving_style == DrivingStyle.CRUISING:
                 target_speed = 8 * 100
-            elif self.urgency == Urgency.NORMAL:
+            elif self.driving_style == DrivingStyle.NORMAL:
                 target_speed = 9 * 100
-            elif self.urgency == Urgency.LATE:
+            elif self.driving_style == DrivingStyle.LATE:
                 target_speed = 10 * 100
             else:
-                raise NotImplementedError('Urgency level not supported')
+                raise NotImplementedError('Driving style not supported')
 
             # Network overfit on speed, plus it's nice to be able to change it,
             # so we just ignore output speed of net
-            desired_throttle = abs(target_speed / max(actual_speed, 1e-3))
-            desired_throttle = min(max(desired_throttle, 0.), 1.)
+            desired_throttle = get_throttle(actual_speed, target_speed)
         log.debug('actual_speed %r' % actual_speed)
 
         # log.info('desired_steering %f', desired_steering)
@@ -319,7 +318,7 @@ class Agent(object):
 def run(experiment, env_id='Deepdrive-v0', should_record=False, net_path=None, should_benchmark=True,
         run_baseline_agent=False, camera_rigs=None, should_rotate_sim_types=False,
         should_record_recovery_from_random_actions=False, render=False, path_follower=False, fps=c.DEFAULT_FPS,
-        net_name=net.ALEXNET_NAME, urgency=Urgency.NORMAL, is_sync=False):
+        net_name=net.ALEXNET_NAME, driving_style=DrivingStyle.NORMAL, is_sync=False):
     if run_baseline_agent:
         net_path = ensure_baseline_weights(net_path)
     reward = 0
@@ -358,13 +357,13 @@ def run(experiment, env_id='Deepdrive-v0', should_record=False, net_path=None, s
     use_sim_start_command_first_lap = c.SIM_START_COMMAND is not None
     gym_env = deepdrive.start(experiment, env_id, should_benchmark=should_benchmark, cameras=cameras,
                               use_sim_start_command=use_sim_start_command_first_lap, render=render, fps=fps,
-                              urgency=urgency, is_sync=is_sync)
+                              driving_style=driving_style, is_sync=is_sync)
     dd_env = gym_env.env
 
     agent = Agent(gym_env.action_space, sess, env=gym_env.env,
                   should_record_recovery_from_random_actions=should_record_recovery_from_random_actions,
                   should_record=should_record, net_path=net_path, random_action_count=4, non_random_action_count=5,
-                  path_follower=path_follower, net_name=net_name, urgency=urgency)
+                  path_follower=path_follower, net_name=net_name, driving_style=driving_style)
     if net_path:
         log.info('Running tensorflow agent checkpoint: %s', net_path)
 
diff --git a/deepdrive.py b/deepdrive.py
index e0a62af..574d4f4 100644
--- a/deepdrive.py
+++ b/deepdrive.py
@@ -7,7 +7,7 @@ import config as c
 import random_name
 
 # noinspection PyUnresolvedReferences
-from gym_deepdrive.envs.deepdrive_gym_env import gym_action as action, Urgency
+from gym_deepdrive.envs.deepdrive_gym_env import gym_action as action, DrivingStyle
 from vendor.openai.baselines.common.continuous_action_wrapper import CombineBoxSpaceWrapper
 
 log = logs.get_log(__name__)
@@ -16,7 +16,7 @@ log = logs.get_log(__name__)
 def start(experiment_name=None, env='Deepdrive-v0', sess=None, start_dashboard=True, should_benchmark=True,
           cameras=None, use_sim_start_command=False, render=False, fps=c.DEFAULT_FPS, combine_box_action_spaces=False,
           is_discrete=False, preprocess_with_tensorflow=False, is_sync=False,
-          urgency=Urgency.NORMAL):
+          driving_style=DrivingStyle.NORMAL):
     env = gym.make(env)
     env.seed(c.RNG_SEED)
 
@@ -33,7 +33,7 @@ def start(experiment_name=None, env='Deepdrive-v0', sess=None, start_dashboard=T
     raw_env.fps = fps
     raw_env.experiment = experiment_name.replace(' ', '_')
     raw_env.period = raw_env.sync_step_time = 1. / fps
-    raw_env.urgency = urgency
+    raw_env.driving_style = driving_style
     raw_env.should_render = render
     raw_env.set_use_sim_start_command(use_sim_start_command)
     raw_env.open_sim()
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index 27f2b58..18bfe7a 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -44,6 +44,7 @@ from dashboard import dashboard_fn, DashboardPub
 
 log = logs.get_log(__name__)
 SPEED_LIMIT_KPH = 64.
+HEAD_START_TIME = 0
 
 
 class Score(object):
@@ -62,6 +63,12 @@ class Score(object):
 
 
 class Action(object):
+    STEERING_INDEX = 0
+    THROTTLE_INDEX = 1
+    BRAKE_INDEX = 2
+    HANDBRAKE_INDEX = 3
+    HAS_CONTROL_INDEX = 4
+
     def __init__(self, steering=0, throttle=0, brake=0, handbrake=0, has_control=True):
         self.steering = steering
         self.throttle = throttle
@@ -81,14 +88,16 @@ class Action(object):
             if isinstance(action[4], list):
                 has_control = action[4][0]
             else:
-                has_control = action[4]
-        handbrake = action[3][0]
+                has_control = action[cls.HANDBRAKE_INDEX]
+        handbrake = action[cls.HANDBRAKE_INDEX][0]
         if handbrake <= 0 or math.isnan(handbrake):
             handbrake = 0
         else:
             handbrake = 1
-        ret = cls(steering=action[0][0], throttle=action[1][0],
-                  brake=action[2][0], handbrake=handbrake, has_control=has_control)
+        ret = cls(steering=action[cls.STEERING_INDEX][0],
+                  throttle=action[cls.THROTTLE_INDEX][0],
+                  brake=action[cls.BRAKE_INDEX][0],
+                  handbrake=handbrake, has_control=has_control)
         return ret
 
 
@@ -134,14 +143,15 @@ class RewardWeighting(object):
                + speed
 
 
-class Urgency(Enum):
+class DrivingStyle(Enum):
     __order__ = 'CRUISING NORMAL LATE EMERGENCY CHASE'
     # TODO: Possibly assign function rather than just weights
-    CRUISING  = RewardWeighting(speed=0.5, progress=0.0, gforce=2.00, lane_deviation=1.50, total_time=0.0)
-    NORMAL    = RewardWeighting(speed=1.0, progress=0.0, gforce=1.00, lane_deviation=1.00, total_time=0.0)
-    LATE      = RewardWeighting(speed=2.0, progress=0.0, gforce=0.50, lane_deviation=0.50, total_time=0.0)
-    EMERGENCY = RewardWeighting(speed=2.0, progress=0.0, gforce=0.75, lane_deviation=0.75, total_time=0.0)
-    CHASE     = RewardWeighting(speed=2.0, progress=0.0, gforce=0.00, lane_deviation=0.00, total_time=0.0)
+    CRUISING   = RewardWeighting(speed=0.5, progress=0.0, gforce=2.00, lane_deviation=1.50, total_time=0.0)
+    NORMAL     = RewardWeighting(speed=1.0, progress=0.0, gforce=1.00, lane_deviation=1.00, total_time=0.0)
+    LATE       = RewardWeighting(speed=2.0, progress=0.0, gforce=0.50, lane_deviation=0.50, total_time=0.0)
+    EMERGENCY  = RewardWeighting(speed=2.0, progress=0.0, gforce=0.75, lane_deviation=0.75, total_time=0.0)
+    CHASE      = RewardWeighting(speed=2.0, progress=0.0, gforce=0.00, lane_deviation=0.00, total_time=0.0)
+    STEER_ONLY = RewardWeighting(speed=1.0, progress=0.0, gforce=0.00, lane_deviation=0.00, total_time=0.0)
 
 
 default_cam = Camera(**c.DEFAULT_CAM)  # TODO: Switch camera dicts to this object
@@ -199,7 +209,7 @@ class DeepDriveEnv(gym.Env):
         self.fps = None  # type: int
         self.period = None  # type: float
         self.experiment = None  # type: str
-        self.urgency = None  # type: Urgency
+        self.driving_style = None  # type: DrivingStyle
 
         if not c.REUSE_OPEN_SIM:
             if utils.get_sim_bin_path() is None:
@@ -445,7 +455,7 @@ class DeepDriveEnv(gym.Env):
 
             self.score.episode_time += (step_time or 0)
 
-            if self.score.episode_time < 2.5:
+            if self.score.episode_time < HEAD_START_TIME:
                 # Give time to get on track after spawn
                 reward = 0
             else:
@@ -478,7 +488,7 @@ class DeepDriveEnv(gym.Env):
             lane_deviation_penalty = DeepDriveRewardCalculator.get_lane_deviation_penalty(
                 obz['distance_to_center_of_lane'], time_passed)
 
-        lane_deviation_penalty *= self.urgency.value.lane_deviation_weight
+        lane_deviation_penalty *= self.driving_style.value.lane_deviation_weight
         self.score.lane_deviation_penalty += lane_deviation_penalty
 
         self.display_stats['lane deviation penalty']['value'] = -lane_deviation_penalty
@@ -495,7 +505,7 @@ class DeepDriveEnv(gym.Env):
                 self.display_stats['g-forces']['total'] = gforces
                 gforce_penalty = DeepDriveRewardCalculator.get_gforce_penalty(gforces, time_passed)
 
-        gforce_penalty *= self.urgency.value.gforce_weight
+        gforce_penalty *= self.driving_style.value.gforce_weight
         self.score.gforce_penalty += gforce_penalty
 
         self.display_stats['gforce penalty']['value'] = -self.score.gforce_penalty
@@ -510,8 +520,8 @@ class DeepDriveEnv(gym.Env):
             self.distance_along_route = dist
             progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress, time_passed)
 
-        progress_reward *= self.urgency.value.progress_weight
-        speed_reward *= self.urgency.value.speed_weight
+        progress_reward *= self.driving_style.value.progress_weight
+        speed_reward *= self.driving_style.value.speed_weight
 
         self.score.progress_reward += progress_reward
         self.score.speed_reward += speed_reward
@@ -527,12 +537,12 @@ class DeepDriveEnv(gym.Env):
 
     def get_time_penalty(self, _obz, time_passed):
         time_penalty = time_passed or 0
-        time_penalty *= self.urgency.value.time_weight
+        time_penalty *= self.driving_style.value.time_weight
         self.score.time_penalty += time_penalty
         return time_penalty
 
     def combine_rewards(self, progress_reward, gforce_penalty, lane_deviation_penalty, time_penalty, speed):
-        return self.urgency.value.combine(progress_reward, gforce_penalty, lane_deviation_penalty, time_penalty, speed)
+        return self.driving_style.value.combine(progress_reward, gforce_penalty, lane_deviation_penalty, time_penalty, speed)
 
     def is_stuck(self, obz):
         start_is_stuck = time.time()
@@ -550,7 +560,7 @@ class DeepDriveEnv(gym.Env):
                 self.steps_crawling_with_throttle_on += 1
             time_crawling = time.time() - self.last_forward_progress_time
             portion_crawling = self.steps_crawling_with_throttle_on / max(1, self.steps_crawling)
-            if self.steps_crawling_with_throttle_on > 20 and time_crawling > 10 and portion_crawling > 0.8:
+            if self.steps_crawling_with_throttle_on > 20 and time_crawling > 2 and portion_crawling > 0.8:
                 log.warn('No progress made while throttle on - assuming stuck and ending episode. steps crawling: %r, '
                          'steps crawling with throttle on: %r, time crawling: %r',
                          self.steps_crawling, self.steps_crawling_with_throttle_on, time_crawling)
diff --git a/main.py b/main.py
index 9fa3607..2152c05 100644
--- a/main.py
+++ b/main.py
@@ -11,7 +11,7 @@ import deepdrive
 import logs
 from agents.dagger import net
 from agents.dagger.agent import ensure_baseline_weights
-from gym_deepdrive.envs.deepdrive_gym_env import Urgency
+from gym_deepdrive.envs.deepdrive_gym_env import DrivingStyle
 
 
 def main():
@@ -47,9 +47,9 @@ def main():
                              'i.e. /home/a/DeepDrive/tensorflow/2018-01-01__11-11-11AM_train/model.ckpt-98331')
     parser.add_argument('--net-type', nargs='?', default=net.ALEXNET_NAME,
                         help='Your model type - i.e. AlexNet or MobileNetV2')
-    parser.add_argument('--urgency', nargs='?', default=Urgency.NORMAL.name.lower(),
+    parser.add_argument('--driving-style', nargs='?', default=DrivingStyle.NORMAL.name.lower(),
                         help='Speed vs comfort prioritization, i.e. ' +
-                             ', '.join([level.name.lower() for level in Urgency]))
+                             ', '.join([level.name.lower() for level in DrivingStyle]))
     parser.add_argument('--resume-train', nargs='?', default=None,
                         help='Name of the tensorflow training session you want to resume within %s, '
                              'i.e. 2018-01-01__11-11-11AM_train' % c.TENSORFLOW_OUT_DIR)
@@ -80,6 +80,8 @@ def main():
         else:
             args.net_path = get_latest_model()
 
+    driving_style = DrivingStyle[args.driving_style.upper()]
+
     if args.train:
         # TODO: Add experiment name here as well, and integrate it into Tensorflow runs, recording names, model checkpoints, etc...
         if args.agent == 'dagger' or args.agent == 'dagger_mobilenet_v2':
@@ -102,7 +104,7 @@ def main():
                 log.info('Bootstrapping from baseline agent')
                 net_path = ensure_baseline_weights(args.net_path)
             train.run(args.env_id, resume_dir=args.resume_train, bootstrap_net_path=net_path, agent_name=args.agent,
-                      render=args.render, camera_rigs=[c.DEFAULT_CAM], is_sync=args.sync)
+                      render=args.render, camera_rigs=[c.DEFAULT_CAM], is_sync=args.sync, driving_style=driving_style)
         else:
             raise Exception('Agent type not recognized')
     elif args.path_follower:
@@ -112,7 +114,7 @@ def main():
         gym_env = None
         try:
             gym_env = deepdrive.start(args.experiment_name, args.env_id, fps=args.fps,
-                                      urgency=Urgency[args.urgency.upper()])
+                                      driving_style=driving_style)
             log.info('Path follower drive mode')
             for episode in range(episode_count):
                 if done:
@@ -142,7 +144,7 @@ def main():
                   run_baseline_agent=args.baseline, render=args.render, camera_rigs=camera_rigs,
                   should_record_recovery_from_random_actions=args.record_recovery_from_random_actions,
                   path_follower=args.path_follower, fps=args.fps, net_name=args.net_type, is_sync=args.sync,
-                  urgency=Urgency[args.urgency.upper()])
+                  driving_style=driving_style)
 
 
 def get_latest_model():
diff --git a/vendor/openai/baselines/ppo2/ppo2.py b/vendor/openai/baselines/ppo2/ppo2.py
index d0d8122..bdedff7 100644
--- a/vendor/openai/baselines/ppo2/ppo2.py
+++ b/vendor/openai/baselines/ppo2/ppo2.py
@@ -8,6 +8,9 @@ import joblib
 import numpy as np
 # noinspection PyPackageRequirements
 import tensorflow as tf
+
+from agents.common import get_throttle
+from gym_deepdrive.envs.deepdrive_gym_env import Action
 from vendor.openai.baselines import logger
 
 from vendor.openai.baselines.common.math_util import explained_variance
@@ -158,6 +161,14 @@ class Runner(object):
         # TODO(py27): Python versions < 3.5 do not support starred expressions in tuples, lists, and sets
         return (*map(sf01, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs)),
             mb_states, epinfos)
+
+    def process_actions(self, actions):
+        action = Action.from_gym(actions)
+        action.throttle = get_throttle(actual_speed=self.obs['speed'], target_speed=(8 * 100))
+        actions = action.as_gym()
+        return actions
+
+
 # obs, returns, masks, actions, values, neglogpacs, states = runner.run()
 
 
diff --git a/vendor/openai/baselines/ppo2/run_deepdrive.py b/vendor/openai/baselines/ppo2/run_deepdrive.py
index 2d03cbf..a44c1bf 100644
--- a/vendor/openai/baselines/ppo2/run_deepdrive.py
+++ b/vendor/openai/baselines/ppo2/run_deepdrive.py
@@ -7,7 +7,7 @@ from vendor.openai.baselines import bench, logger
 from vendor.openai.baselines.common.cmd_util import continuous_mountain_car_arg_parser
 
 
-def train(env, num_timesteps, seed, sess=None, is_discrete=True):
+def train(env, seed, sess=None, is_discrete=True):
     from vendor.openai.baselines.common.misc_util import set_global_seeds
     from vendor.openai.baselines.common.vec_env.vec_normalize import VecNormalize
     from vendor.openai.baselines.ppo2 import ppo2
@@ -45,15 +45,15 @@ def train(env, num_timesteps, seed, sess=None, is_discrete=True):
 
     ppo2.learn(policy=policy,
                env=env,
-               nsteps=40,
-               nminibatches=1,  # Sweet spot is between 16 and 64 for continuous mountain car @55fps
+               nsteps=80,
+               nminibatches=2,  # Sweet spot is between 16 and 64 for continuous mountain car @55fps
                lam=0.95,
                gamma=0.99,
                save_interval=1,
                noptepochs=3,
                log_interval=1,
                ent_coef=0.0,
-               lr=lambda f: f * 2.5e-4,
+               lr=lambda f: f * 2.5e-2,
                cliprange=lambda f: f * 0.1,
-               total_timesteps=num_timesteps)
+               total_timesteps=int(1e4))