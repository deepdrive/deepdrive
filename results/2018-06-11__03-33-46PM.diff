diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index e6767ca..3433c8e 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -18,6 +18,8 @@ class BootstrapRLGymEnv(gym.Wrapper):
         self.dagger_agent = dagger_agent
         self.previous_obz = None
 
+        self.simple_test = c.SIMPLE_PPO
+
         # One thing we need to do here is to make each action a bi-modal guassian to avoid averaging 50/50 decisions
         # i.e. half the time we veer left, half the time veer right - but on average this is go straight and can run us
         # into an obstacle. right now the DiagGaussianPd is just adding up errors which would not be the right
@@ -25,10 +27,15 @@ class BootstrapRLGymEnv(gym.Wrapper):
         # independent which is not the case (steering at higher speeds causes more acceleration a = v**2/r),
         # so that may be a problem as well.
 
+        if self.simple_test:
+            shape = (5,)
+        else:
+            shape = (dagger_agent.net.num_last_hidden + dagger_agent.net.num_targets,)
+
         self.observation_space = spaces.Box(low=np.finfo(np.float32).min,
                                             high=np.finfo(np.float32).max,
                                             # shape=(c.ALEXNET_FC7 + c.NUM_TARGETS,),
-                                            shape=(dagger_agent.net.num_last_hidden + dagger_agent.net.num_targets,),
+                                            shape=shape,
                                             dtype=np.float32)
 
     def step(self, action):
@@ -37,14 +44,16 @@ class BootstrapRLGymEnv(gym.Wrapper):
             # is not disincentivized by gforce penalty.
             action[Action.THROTTLE_INDEX] = get_throttle(actual_speed=self.previous_obz['speed'],
                                                          target_speed=(8 * 100))
-
         obz, reward, done, info = self.env.step(action)
         self.previous_obz = obz
         action, net_out = self.dagger_agent.act(obz, reward, done)
         if net_out is None:
             obz = None
         else:
-            obz = np.concatenate((np.squeeze(net_out[0]), np.squeeze(net_out[1])))
+            if self.simple_test:
+                obz = np.array([np.squeeze(a) for a in action])
+            else:
+                obz = np.concatenate((np.squeeze(net_out[0]), np.squeeze(net_out[1])))
         return obz, reward, done, info
 
     def reset(self):
@@ -88,7 +97,14 @@ def run(env_id, bootstrap_net_path,
             # Wrap step so we get the pretrained layer activations rather than pixels for our observation
             bootstrap_gym_env = BootstrapRLGymEnv(dagger_gym_env, dagger_agent)
 
-            train(bootstrap_gym_env, seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete)
+            if c.SIMPLE_PPO:
+                mlp_width = 5
+                minibatch_steps = 1
+            else:
+                minibatch_steps = 80
+                mlp_width = 64
+            train(bootstrap_gym_env, seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete,
+                  minibatch_steps=minibatch_steps, mlp_width=mlp_width)
     #
     # action = deepdrive.action()
     # while not done:
diff --git a/config.py b/config.py
index d862a80..dde6610 100644
--- a/config.py
+++ b/config.py
@@ -121,3 +121,7 @@ DEFAULT_CAM = dict(name='forward cam 227x227 60 FOV', field_of_view=60, capture_
                    relative_rotation=[0.0, 0.0, 0.0])
 
 DEFAULT_FPS = 8
+
+
+# Experimental stuff - not worth passing as main.py args yet, but better for reproducing to put here than in os.environ
+SIMPLE_PPO = True
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index 1acdb0c..4159e46 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -775,7 +775,7 @@ class DeepDriveEnv(gym.Env):
             self.change_has_control(action.has_control)
 
         if action.handbrake:
-            log.warn('Not expecting any handbraking right now! What\'s happening?! Disabling - hack :D')
+            log.debug('Not expecting any handbraking right now! What\'s happening?! Disabling - hack :D')
             action.handbrake = False
 
         action.clip()
diff --git a/vendor/openai/baselines/a2c/utils.py b/vendor/openai/baselines/a2c/utils.py
index 0964af8..146c49a 100644
--- a/vendor/openai/baselines/a2c/utils.py
+++ b/vendor/openai/baselines/a2c/utils.py
@@ -62,7 +62,10 @@ def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):
     with tf.variable_scope(scope):
         nin = x.get_shape()[1].value
         w = tf.get_variable("w", [nin, nh], initializer=ortho_init(init_scale))
+        w = tf.Print(w, ['w ', scope, w], summarize=100)
         b = tf.get_variable("b", [nh], initializer=tf.constant_initializer(init_bias))
+        b = tf.Print(b, ['b ', scope, w], summarize=100)
+
         return tf.matmul(x, w)+b
 
 def batch_to_seq(h, nbatch, nsteps, flat=False):
diff --git a/vendor/openai/baselines/ppo2/policies.py b/vendor/openai/baselines/ppo2/policies.py
index 6565d3d..5ea4b41 100644
--- a/vendor/openai/baselines/ppo2/policies.py
+++ b/vendor/openai/baselines/ppo2/policies.py
@@ -2,6 +2,7 @@ import numpy as np
 import tensorflow as tf
 from vendor.openai.baselines.common.distributions import make_pdtype
 from gym import spaces
+import config as c
 
 from vendor.openai.baselines.a2c.utils import conv, fc, conv_to_fc, batch_to_seq, seq_to_batch, lstm, lnlstm
 from vendor.openai.baselines.ppo2.ppo2 import TF_VAR_SCOPE
@@ -210,6 +211,7 @@ class MlpPolicy(object):
 
         pdparam = tf.concat([pi, pi * 0.0 + logstd], axis=1)
 
+        self.p_h1 = p_h1
         self.pdtype = make_pdtype(ac_space)
         self.pd = self.pdtype.pdfromflat(pdparam)
 
@@ -225,7 +227,11 @@ class MlpPolicy(object):
         self.initial_state = None
 
         def step(ob, *_args, **_kwargs):
-            a, v, neglogp = sess.run([a0, vf, neglogp0], {X:ob})
+            if c.SIMPLE_PPO:
+                a, v, neglogp, p_w0 = sess.run([a0, vf, neglogp0, self.p_h1], {X:ob})
+                print('pw0', p_w0)
+            else:
+                a, v, neglogp = sess.run([a0, vf, neglogp0], {X: ob})
 
             # For deepdrive we expect outputs to be between -1 and 1 - let's just max out actions for now
             # a = np.tanh(a)
diff --git a/vendor/openai/baselines/ppo2/run_deepdrive.py b/vendor/openai/baselines/ppo2/run_deepdrive.py
index a6a17ca..589b1a7 100644
--- a/vendor/openai/baselines/ppo2/run_deepdrive.py
+++ b/vendor/openai/baselines/ppo2/run_deepdrive.py
@@ -2,12 +2,14 @@
 
 import os
 
+import config as c
+
 from vendor.openai.baselines import bench, logger
 
 from vendor.openai.baselines.common.cmd_util import continuous_mountain_car_arg_parser
 
 
-def train(env, seed, sess=None, is_discrete=True, minibatch_steps=80, mlp_width=64):
+def train(env, seed, sess=None, is_discrete=True, minibatch_steps=None, mlp_width=None):
     from vendor.openai.baselines.common.misc_util import set_global_seeds
     from vendor.openai.baselines.common.vec_env.vec_normalize import VecNormalize
     from vendor.openai.baselines.ppo2 import ppo2
@@ -26,8 +28,11 @@ def train(env, seed, sess=None, is_discrete=True, minibatch_steps=80, mlp_width=
         tf.Session(config=config).__enter__()
 
     env = DummyVecEnv(envs=[env])
-    # env = VecNormalize(env, ob=False)
-    env = VecNormalize(env)
+
+    if c.SIMPLE_PPO:
+        env = VecNormalize(env, ob=False)
+    else:
+        env = VecNormalize(env)
 
     set_global_seeds(seed)
     if is_discrete:
@@ -51,7 +56,7 @@ def train(env, seed, sess=None, is_discrete=True, minibatch_steps=80, mlp_width=
                noptepochs=3,
                log_interval=1,
                ent_coef=0.0,
-               lr=lambda f: f * 2.5e-2,
+               lr=lambda f: f * 2.5e-4,
                cliprange=lambda f: f * 0.1,
                total_timesteps=int(1e5),
                mlp_width=mlp_width)