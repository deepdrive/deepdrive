diff --git a/config.py b/config.py
index 8b4cbe9..28967b9 100644
--- a/config.py
+++ b/config.py
@@ -73,6 +73,7 @@ def _ensure_python_bin_config():
         _dpbf.write(sys.executable)
 
 
+ROOT_DIR = os.path.dirname(os.path.realpath(__file__))
 DEEPDRIVE_DIR = os.environ.get('DEEPDRIVE_DIR')
 DEEPDRIVE_CONFIG_DIR = os.path.expanduser('~') + '/.deepdrive'
 os.makedirs(DEEPDRIVE_CONFIG_DIR, exist_ok=True)
@@ -86,7 +87,7 @@ DATE_STR = datetime.now().strftime(DIR_DATE_FORMAT)
 RECORDING_DIR = os.environ.get('DEEPDRIVE_RECORDING_DIR') or os.path.join(DEEPDRIVE_DIR, 'recordings')
 GYM_DIR = os.path.join(DEEPDRIVE_DIR, 'gym')
 LOG_DIR = os.path.join(DEEPDRIVE_DIR, 'log')
-BENCHMARK_DIR = os.path.join(DEEPDRIVE_DIR, 'benchmark')
+RESULTS_DIR = os.path.join(ROOT_DIR, 'results')
 TENSORFLOW_OUT_DIR = os.path.join(DEEPDRIVE_DIR, 'tensorflow')
 WEIGHTS_DIR = os.path.join(DEEPDRIVE_DIR, 'weights')
 
@@ -122,10 +123,6 @@ DEFAULT_CAM = dict(name='forward cam 227x227 60 FOV', field_of_view=60, capture_
 
 DEFAULT_FPS = 8
 
-
-# Experimental stuff - not worth passing as main.py args yet, but better for reproducing to put here than in os.environ
-SIMPLE_PPO = False
-
 try:
     import tensorflow
 except ImportError:
@@ -133,3 +130,8 @@ except ImportError:
 else:
     TENSORFLOW_AVAILABLE = True
 
+
+# Not passing through main.py args yet, but better for reproducing to put here than in os.environ
+SIMPLE_PPO = False
+PPO_RESUME_PATH = '/home/a/baselines_results/openai-2018-06-17-17-48-24-795338/checkpoints/00001'
+TEST_PPO = True
\ No newline at end of file
diff --git a/deepdrive.py b/deepdrive.py
index 56b0fcb..2c6058f 100644
--- a/deepdrive.py
+++ b/deepdrive.py
@@ -50,7 +50,7 @@ def start(experiment_name=None, env='Deepdrive-v0', sess=None, start_dashboard=T
     if start_dashboard:
         raw_env.start_dashboard()
     if should_benchmark:
-        log.info('Benchmarking enabled - will save results to %s', c.BENCHMARK_DIR)
+        log.info('Benchmarking enabled - will save results to %s', c.RESULTS_DIR)
         raw_env.init_benchmarking()
     env.reset()
     return env
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index e875f29..1d37dfa 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -185,11 +185,16 @@ class DrivingStyle(Enum):
     across all the objectives. Perhaps inputting the different components running averages or real-time values to
     a recurrent part of the model would allow it to balance the objectives through SGD rather than the above
     simple linear tweaking.
+
+    - After some experimentation, seems like we may not need this yet. Observation normalization was causing the
+    motivating problem by learning too slow. Optimization does find a way. I think distributional RL may be helpful here
+    especially if we can get dimensions for all the compoenents of the reward. Also a novelty bonus on
+    (observation,action) or (game-state,action) would be helpful most likely to avoid local minima.
     """
     __order__ = 'CRUISING NORMAL LATE EMERGENCY CHASE'
     # TODO: Possibly assign function rather than just weights
     CRUISING   = RewardWeighting(speed=0.5, progress=0.0, gforce=2.00, lane_deviation=1.50, total_time=0.0)
-    NORMAL     = RewardWeighting(speed=1.0, progress=0.0, gforce=0.00, lane_deviation=0.00, total_time=0.0)
+    NORMAL     = RewardWeighting(speed=1.0, progress=0.0, gforce=0.00, lane_deviation=0.10, total_time=0.0)
     LATE       = RewardWeighting(speed=2.0, progress=0.0, gforce=0.50, lane_deviation=0.50, total_time=0.0)
     EMERGENCY  = RewardWeighting(speed=2.0, progress=0.0, gforce=0.75, lane_deviation=0.75, total_time=0.0)
     CHASE      = RewardWeighting(speed=2.0, progress=0.0, gforce=0.00, lane_deviation=0.00, total_time=0.0)
@@ -251,7 +256,7 @@ class DeepDriveEnv(gym.Env):
         self.fps = None  # type: int
         self.period = None  # type: float
         self.experiment = None  # type: str
-        self.driving_style = None  # type: DrivingStyle
+        self.driving_style = None  # type: DrivingStyle`
         self.reset_returns_zero = None  # type: bool
         self.started_driving_wrong_way_time = None  # type: bool
         self.previous_distance_along_route = None  # type: bool
@@ -395,7 +400,7 @@ class DeepDriveEnv(gym.Env):
 
     def init_benchmarking(self):
         self.should_benchmark = True
-        os.makedirs(c.BENCHMARK_DIR, exist_ok=True)
+        os.makedirs(c.RESULTS_DIR, exist_ok=True)
 
     def init_pyglet(self, cameras):
         q = Queue(maxsize=1)
@@ -672,9 +677,9 @@ class DeepDriveEnv(gym.Env):
         std = np.std(totals)
         log.info('benchmark lap #%d score: %f - average: %f', len(self.trial_scores), self.score.total, average)
         file_prefix = self.experiment + '_' if self.experiment else ''
-        filename = os.path.join(c.BENCHMARK_DIR, '%s%s.csv' % (file_prefix, c.DATE_STR))
+        filename = os.path.join(c.RESULTS_DIR, '%s%s.csv' % (file_prefix, c.DATE_STR))
         diff_filename = '%s%s.diff' % (file_prefix, c.DATE_STR)
-        diff_filepath = os.path.join(c.BENCHMARK_DIR, diff_filename)
+        diff_filepath = os.path.join(c.RESULTS_DIR, diff_filename)
 
         if self.git_diff is not None:
             with open(diff_filepath, 'w') as diff_file:
@@ -866,6 +871,8 @@ class DeepDriveEnv(gym.Env):
                                                                        action.brake, action.handbrake)
             log.debug('sync step took %fs',  time.time() - sync_start)
         else:
+            if c.PPO_RESUME_PATH:
+                action.throttle = action.throttle * 0.90  # OmegaHack to deal with sync vs async
             deepdrive_client.set_control_values(self.client_id, steering=action.steering, throttle=action.throttle,
                                                 brake=action.brake, handbrake=action.handbrake)
 
diff --git a/vendor/openai/baselines/ppo2/ppo2.py b/vendor/openai/baselines/ppo2/ppo2.py
index 922abc6..7dc2fea 100644
--- a/vendor/openai/baselines/ppo2/ppo2.py
+++ b/vendor/openai/baselines/ppo2/ppo2.py
@@ -15,6 +15,8 @@ from vendor.openai.baselines import logger
 
 from vendor.openai.baselines.common.math_util import explained_variance
 
+import config as c
+
 TF_VAR_SCOPE = 'ppo2model'
 
 
@@ -249,6 +251,9 @@ def learn(*, policy, env, nsteps, total_timesteps, ent_coef, lr,
         with open(osp.join(logger.get_dir(), 'make_model.pkl'), 'wb') as fh:
             fh.write(cloudpickle.dumps(make_model))
     model = make_model()
+    if c.PPO_RESUME_PATH is not None:
+        model.load(c.PPO_RESUME_PATH)
+
     runner = Runner(env=env, model=model, nsteps=nsteps, gamma=gamma, lam=lam)
 
     epinfobuf = deque(maxlen=100)
@@ -262,7 +267,12 @@ def learn(*, policy, env, nsteps, total_timesteps, ent_coef, lr,
         frac = 1.0 - (update - 1.0) / nupdates
         lrnow = lr(frac)
         cliprangenow = cliprange(frac)
+
         obs, returns, masks, actions, values, neglogpacs, states, epinfos = runner.run() #pylint: disable=E0632
+
+        if c.TEST_PPO:
+            continue
+
         epinfobuf.extend(epinfos)
         mblossvals = []
         if states is None: # nonrecurrent version